# ğŸ“ˆ Pipeline de Web Scraping para AnÃ¡lise Financeira

Este projeto Ã© uma pipeline automatizada que coleta, processa e armazena dados financeiros histÃ³ricos de aÃ§Ãµes e notÃ­cias. O objetivo Ã© mostrar minhas habilidades em web scraping, API pÃºblica, manipulaÃ§Ã£o de dados e integraÃ§Ã£o com banco de dados.

## ğŸ” DescriÃ§Ã£o

A pipeline realiza as seguintes etapas:

1. **ExtraÃ§Ã£o de Dados**
   - **NotÃ­cias**: Coleta notÃ­cias financeiras de fontes como G1 e Folha de SÃ£o Paulo.
   - **Dados HistÃ³ricos**: ObtÃ©m sÃ©ries temporais de aÃ§Ãµes utilizando a API pÃºblica do Yahoo Finance (`yfinance`).

2. **Processamento e Limpeza**
   - DeduplicaÃ§Ã£o de registros.
   - NormalizaÃ§Ã£o de datas e valores numÃ©ricos.
   - RemoÃ§Ã£o de URLs invÃ¡lidas ou duplicadas.

3. **Armazenamento**
   - Dados limpos sÃ£o salvos em um banco SQLite local (`pipeline.db`).

## âš™ï¸ Tecnologias Utilizadas

- Python 3.9+
- `pandas` para manipulaÃ§Ã£o de dados
- `requests` e `BeautifulSoup` para scraping de notÃ­cias
- `selenium` e `webdriver_manager` para scraping de pÃ¡ginas dinÃ¢micas
- `yfinance` para dados histÃ³ricos de aÃ§Ãµes
- SQLite para armazenamento local
- Logging com `logging` do Python

## ğŸ“‚ Estrutura do Projeto

```bash
pipeline_web_scraping/
â”‚
â”œâ”€ src/
â”‚ â”œâ”€ extract_news.py # ExtraÃ§Ã£o de notÃ­cias
â”‚ â”œâ”€ extract_yahoo.py # ExtraÃ§Ã£o de dados histÃ³ricos via Yahoo Finance
â”‚ â”œâ”€ transform.py # Limpeza e transformaÃ§Ã£o dos dados
â”‚ â”œâ”€ load.py # PersistÃªncia no banco SQLite
â”‚ â”œâ”€ pipeline.py # ExecuÃ§Ã£o da pipeline
â”‚ â”œâ”€ task.py # Classe base para tarefas
| â”œâ”€init_db.py
â”‚ â””â”€ logger_config.py # ConfiguraÃ§Ã£o de logs
â”‚
â”œâ”€ data/
â”‚ â”œâ”€ raw/ # Dados brutos coletados
â”‚ â”œâ”€ processed/ # Dados limpos e processados
â”‚ â””â”€ database/ # Banco SQLite
â”œâ”€notbooks
  â”œâ”€main.ipynb # Script principal da pipeline
```


## ğŸš€ Como Executar

 Clone o repositÃ³rio:

```bash
git clone https://github.com/dantedod/Pipeline_web_scraping_.git
cd Pipeline_web_scraping
````
Criar ambiente virtual
```bash
# Criar
python -m venv .venv
Windows (cmd):
.venv\Scripts\activate
# Ativar (macOS/Linux)
source .venv/bin/activate
```
Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```
Inicie o Jupyer e execute o pipeline
```bash
jupyter notebook
```
Ou apenas rode manualmente no arquivo main.ipynb

## Autor
[Dante Dantas](https://www.linkedin.com/in/dantedod/) â€“ Trabalho de portfÃ³lio e demonstraÃ§Ã£o de habilidades em Python, Web Scraping e Engenharia de Dados.
